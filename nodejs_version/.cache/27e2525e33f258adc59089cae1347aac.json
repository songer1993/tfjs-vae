{"dependencies":[{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\package.json","includedInParent":true,"mtime":1528200878026},{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\.babelrc","includedInParent":true,"mtime":1528197961732},{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1524501157000},{"name":"../backend/tfjs_backend","loc":{"line":13,"column":16}},{"name":"../engine/topology","loc":{"line":14,"column":25}},{"name":"../errors","loc":{"line":15,"column":23}},{"name":"../utils/generic_utils","loc":{"line":16,"column":28}},{"name":"./serialization","loc":{"line":17,"column":30}}],"generated":{"js":"\"use strict\";\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = Object.setPrototypeOf ||\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar K = require(\"../backend/tfjs_backend\");\nvar topology_1 = require(\"../engine/topology\");\nvar errors_1 = require(\"../errors\");\nvar generic_utils = require(\"../utils/generic_utils\");\nvar serialization_1 = require(\"./serialization\");\nvar Wrapper = (function (_super) {\n    __extends(Wrapper, _super);\n    function Wrapper(config) {\n        var _this = _super.call(this, config) || this;\n        _this.layer = config.layer;\n        return _this;\n    }\n    Wrapper.prototype.build = function (inputShape) {\n        this.built = true;\n    };\n    Object.defineProperty(Wrapper.prototype, \"trainable\", {\n        get: function () {\n            if (this.layer != null) {\n                return this.layer.trainable;\n            }\n            else {\n                return false;\n            }\n        },\n        set: function (value) {\n            if (this.layer != null) {\n                this.layer.trainable = value;\n            }\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"trainableWeights\", {\n        get: function () {\n            return this.layer.trainableWeights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"nonTrainableWeights\", {\n        get: function () {\n            return this.layer.nonTrainableWeights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"updates\", {\n        get: function () {\n            return this.layer._updates;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"losses\", {\n        get: function () {\n            return this.layer.losses;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Wrapper.prototype.getWeights = function () {\n        return this.layer.getWeights();\n    };\n    Wrapper.prototype.setWeights = function (weights) {\n        this.layer.setWeights(weights);\n    };\n    Wrapper.prototype.getConfig = function () {\n        var config = {\n            'layer': {\n                'className': this.layer.getClassName(),\n                'config': this.layer.getConfig(),\n            }\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    Wrapper.fromConfig = function (cls, config, customObjects) {\n        if (customObjects === void 0) { customObjects = {}; }\n        var layerConfig = config['layer'];\n        var layer = serialization_1.deserialize(layerConfig, customObjects);\n        delete config['layer'];\n        var newConfig = { layer: layer };\n        Object.assign(newConfig, config);\n        return new cls(newConfig);\n    };\n    return Wrapper;\n}(topology_1.Layer));\nexports.Wrapper = Wrapper;\nvar TimeDistributed = (function (_super) {\n    __extends(TimeDistributed, _super);\n    function TimeDistributed(config) {\n        var _this = _super.call(this, config) || this;\n        _this.supportsMasking = true;\n        return _this;\n    }\n    TimeDistributed.prototype.build = function (inputShape) {\n        inputShape = generic_utils.getExactlyOneShape(inputShape);\n        if (inputShape.length < 3) {\n            throw new errors_1.ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received \" +\n                (\"input shape \" + JSON.stringify(inputShape)));\n        }\n        this.inputSpec = [{ shape: inputShape }];\n        var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (!this.layer.built) {\n            this.layer.build(childInputShape);\n            this.layer.built = true;\n        }\n        _super.prototype.build.call(this, inputShape);\n    };\n    TimeDistributed.prototype.computeOutputShape = function (inputShape) {\n        inputShape = generic_utils.getExactlyOneShape(inputShape);\n        var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        var childOutputShape = this.layer.computeOutputShape(childInputShape);\n        var timesteps = inputShape[1];\n        return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    };\n    TimeDistributed.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        inputs = generic_utils.getExactlyOneTensor(inputs);\n        var step = function (inputs, states) {\n            var output = _this.layer.call(inputs, kwargs);\n            return [output, []];\n        };\n        var rnnOutputs = K.rnn(step, inputs, [], false, null, null, false, inputs.shape[1]);\n        var y = rnnOutputs[1];\n        return y;\n    };\n    TimeDistributed.prototype.getClassName = function () {\n        return 'TimeDistributed';\n    };\n    return TimeDistributed;\n}(Wrapper));\nexports.TimeDistributed = TimeDistributed;\ngeneric_utils.ClassNameMap.register('TimeDistributed', TimeDistributed);\nvar BidirectionalMergeMode;\n(function (BidirectionalMergeMode) {\n    BidirectionalMergeMode[BidirectionalMergeMode[\"SUM\"] = 0] = \"SUM\";\n    BidirectionalMergeMode[BidirectionalMergeMode[\"MUL\"] = 1] = \"MUL\";\n    BidirectionalMergeMode[BidirectionalMergeMode[\"CONCAT\"] = 2] = \"CONCAT\";\n    BidirectionalMergeMode[BidirectionalMergeMode[\"AVE\"] = 3] = \"AVE\";\n})(BidirectionalMergeMode = exports.BidirectionalMergeMode || (exports.BidirectionalMergeMode = {}));\ngeneric_utils.SerializableEnumRegistry.register('merge_mode', {\n    'sum': BidirectionalMergeMode.SUM,\n    'mul': BidirectionalMergeMode.MUL,\n    'concat': BidirectionalMergeMode.CONCAT,\n    'ave': BidirectionalMergeMode.AVE,\n});\nvar Bidirectional = (function (_super) {\n    __extends(Bidirectional, _super);\n    function Bidirectional(config) {\n        var _this = _super.call(this, config) || this;\n        _this.forwardLayer = config.layer;\n        var layerConfig = config.layer.getConfig();\n        layerConfig['goBackwards'] =\n            layerConfig['goBackwards'] === true ? false : true;\n        _this.backwardLayer =\n            serialization_1.deserialize({ className: config.layer.getClassName(), config: layerConfig });\n        _this.forwardLayer.name = 'forward_' + _this.forwardLayer.name;\n        _this.backwardLayer.name = 'backward_' + _this.backwardLayer.name;\n        _this.mergeMode = config.mergeMode;\n        if (config.weights) {\n            throw new errors_1.NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n        }\n        _this._stateful = config.layer.stateful;\n        _this.returnSequences = config.layer.returnSequences;\n        _this.returnState = config.layer.returnState;\n        _this.supportsMasking = true;\n        _this._trainable = true;\n        _this.inputSpec = config.layer.inputSpec;\n        return _this;\n    }\n    Object.defineProperty(Bidirectional.prototype, \"trainable\", {\n        get: function () {\n            return this._trainable;\n        },\n        set: function (value) {\n            this._trainable = value;\n            if (this.forwardLayer != null) {\n                this.forwardLayer.trainable = value;\n            }\n            if (this.backwardLayer != null) {\n                this.backwardLayer.trainable = value;\n            }\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Bidirectional.prototype.getWeights = function () {\n        return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    };\n    Bidirectional.prototype.setWeights = function (weights) {\n        var numWeights = weights.length;\n        var numeightsOver2 = Math.floor(numWeights / 2);\n        this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n        this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    };\n    Bidirectional.prototype.computeOutputShape = function (inputShape) {\n        var layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n        if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n            layerShapes = [layerShapes];\n        }\n        layerShapes = layerShapes;\n        var outputShape;\n        var outputShapes;\n        var stateShape;\n        if (this.returnState) {\n            stateShape = layerShapes.slice(1);\n            outputShape = layerShapes[0];\n        }\n        else {\n            outputShape = layerShapes[0];\n        }\n        outputShape = outputShape;\n        if (this.mergeMode === BidirectionalMergeMode.CONCAT) {\n            outputShape[outputShape.length - 1] *= 2;\n            outputShapes = [outputShape];\n        }\n        else if (this.mergeMode == null) {\n            outputShapes = [outputShape, outputShape.slice()];\n        }\n        else {\n            outputShapes = [outputShape];\n        }\n        if (this.returnState) {\n            if (this.mergeMode == null) {\n                return outputShapes.concat(stateShape).concat(stateShape.slice());\n            }\n            return [outputShape].concat(stateShape).concat(stateShape.slice());\n        }\n        return generic_utils.singletonOrArray(outputShapes);\n    };\n    Bidirectional.prototype.apply = function (inputs, kwargs) {\n        var initialState = null;\n        if (kwargs != null) {\n            initialState = kwargs['initialState'];\n        }\n        if (Array.isArray(inputs)) {\n            initialState = inputs.slice(1);\n            inputs = inputs[0];\n        }\n        if (initialState == null || initialState.length === 0) {\n            var applyOutputs = _super.prototype.apply.call(this, inputs, kwargs);\n            return applyOutputs;\n        }\n        else {\n            throw new errors_1.NotImplementedError('The support for initial states is not implemented for ' +\n                'Bidirectional layers yet.');\n        }\n    };\n    Bidirectional.prototype.call = function (inputs, kwargs) {\n        if (kwargs['mask'] != null) {\n            throw new errors_1.NotImplementedError('The support for masking is not implemented for ' +\n                'Bidirectional layers yet.');\n        }\n        if (kwargs['initialState'] != null) {\n            throw new errors_1.NotImplementedError('The support for initial states is not implemented for ' +\n                'Bidirectional layers yet.');\n        }\n        var y = this.forwardLayer.call(inputs, kwargs);\n        var yRev = this.backwardLayer.call(inputs, kwargs);\n        var states;\n        if (this.returnState) {\n            if (Array.isArray(y)) {\n                states = y.slice(1).concat(yRev.slice(1));\n            }\n            else {\n            }\n            y = y[0];\n            yRev = yRev[0];\n        }\n        if (this.returnSequences) {\n            yRev = K.reverse(yRev, 1);\n        }\n        var output;\n        if (this.mergeMode === BidirectionalMergeMode.CONCAT) {\n            output = K.concatenate([y, yRev]);\n        }\n        else if (this.mergeMode === BidirectionalMergeMode.SUM) {\n            output = K.add(y, yRev);\n        }\n        else if (this.mergeMode === BidirectionalMergeMode.AVE) {\n            output = K.scalarTimesArray(K.getScalar(0.5), K.add(y, yRev));\n        }\n        else if (this.mergeMode === BidirectionalMergeMode.MUL) {\n            output = K.multiply(y, yRev);\n        }\n        else if (this.mergeMode == null) {\n            output = [y, yRev];\n        }\n        if (this.returnState) {\n            if (this.mergeMode == null) {\n                return output.concat(states);\n            }\n            return [output].concat(states);\n        }\n        return output;\n    };\n    Bidirectional.prototype.resetStates = function (states) {\n        this.forwardLayer.resetStates();\n        this.backwardLayer.resetStates();\n    };\n    Bidirectional.prototype.build = function (inputShape) {\n        var _this = this;\n        K.nameScope(this.forwardLayer.name, function () {\n            _this.forwardLayer.build(inputShape);\n        });\n        K.nameScope(this.backwardLayer.name, function () {\n            _this.backwardLayer.build(inputShape);\n        });\n        this.built = true;\n    };\n    Object.defineProperty(Bidirectional.prototype, \"trainableWeights\", {\n        get: function () {\n            return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Bidirectional.prototype, \"nonTrainableWeights\", {\n        get: function () {\n            return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Bidirectional.prototype.getClassName = function () {\n        return 'Bidirectional';\n    };\n    return Bidirectional;\n}(Wrapper));\nexports.Bidirectional = Bidirectional;\ngeneric_utils.ClassNameMap.register('Bidirectional', Bidirectional);\n"},"hash":"8c5aba8e96923300a0005be65f2934be","cacheData":{"env":{}}}