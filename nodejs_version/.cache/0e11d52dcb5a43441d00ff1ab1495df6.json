{"dependencies":[{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\package.json","includedInParent":true,"mtime":1528200878026},{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\.babelrc","includedInParent":true,"mtime":1528197961732},{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1524501157000},{"name":"@tensorflow/tfjs-core","loc":{"line":4,"column":26}},{"name":"../common","loc":{"line":5,"column":23}},{"name":"../errors","loc":{"line":6,"column":23}},{"name":"../types","loc":{"line":7,"column":22}},{"name":"../utils/generic_utils","loc":{"line":8,"column":30}},{"name":"../utils/math_utils","loc":{"line":9,"column":25}},{"name":"./common","loc":{"line":11,"column":23}}],"generated":{"js":"\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar tfc = require(\"@tensorflow/tfjs-core\");\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar common_1 = require(\"../common\");\nvar errors_1 = require(\"../errors\");\nvar types_1 = require(\"../types\");\nvar generic_utils_1 = require(\"../utils/generic_utils\");\nvar math_utils = require(\"../utils/math_utils\");\nvar common_2 = require(\"./common\");\nvar common_3 = require(\"./common\");\nvar backend = 'webgl';\nvar DEFAULT_DTYPE = types_1.DType.float32;\nfunction disposeScalarCache() {\n    for (var typeKey in scalarCache) {\n        for (var key in scalarCache[typeKey]) {\n            scalarCache[typeKey][key].dispose();\n            delete scalarCache[typeKey][key];\n        }\n    }\n}\nexports.disposeScalarCache = disposeScalarCache;\nfunction setBackend(requestedBackend) {\n    tfc.setBackend(requestedBackend);\n    backend = requestedBackend;\n    disposeScalarCache();\n}\nexports.setBackend = setBackend;\nfunction getBackend() {\n    return backend;\n}\nexports.getBackend = getBackend;\nfunction keep(x) {\n    return tfc.keep(x);\n}\nexports.keep = keep;\nvar scalarCache = {\n    float32: {},\n    int32: {}\n};\nfunction getScalar(value, dtype) {\n    if (dtype === undefined) {\n        dtype = DEFAULT_DTYPE;\n    }\n    if (scalarCache[dtype][value] == null) {\n        scalarCache[dtype][value] = tfjs_core_1.scalar(value, dtype);\n        tfc.keep(scalarCache[dtype][value]);\n    }\n    return scalarCache[dtype][value];\n}\nexports.getScalar = getScalar;\nexports.epsilon = common_2.epsilon;\nfunction isBackendSymbolic() {\n    return false;\n}\nexports.isBackendSymbolic = isBackendSymbolic;\nfunction shape(x) {\n    return x.shape;\n}\nexports.shape = shape;\nfunction intShape(x) {\n    return x.shape;\n}\nexports.intShape = intShape;\nfunction ndim(x) {\n    return x.shape.length;\n}\nexports.ndim = ndim;\nfunction dtype(x) {\n    return (x instanceof tfjs_core_1.Tensor) ? DEFAULT_DTYPE : x.dtype;\n}\nexports.dtype = dtype;\nfunction normalizeAxis(x, axis) {\n    if (axis == null) {\n        return axis;\n    }\n    var xShape = shape(x);\n    if (Array.isArray(axis)) {\n        return axis.map(function (thisAxis) { return generic_utils_1.pyNormalizeArrayIndex(xShape, thisAxis); });\n    }\n    return generic_utils_1.pyNormalizeArrayIndex(xShape, axis);\n}\nexports.normalizeAxis = normalizeAxis;\nfunction countParams(x) {\n    var shape = x.shape;\n    if (shape.length > 0) {\n        return shape.reduce(function (a, b) { return a * b; });\n    }\n    else {\n        return 1;\n    }\n}\nexports.countParams = countParams;\nfunction cast(x, dtype) {\n    return x.asType(dtype);\n}\nexports.cast = cast;\nfunction reshape(x, shape) {\n    return x.reshape(shape);\n}\nexports.reshape = reshape;\nfunction transpose(x, perm) {\n    return tfc.transpose(x, perm);\n}\nexports.transpose = transpose;\nexports.permuteDimensions = transpose;\nfunction reverse(x, axes) {\n    return tfc.reverse(x, axes);\n}\nexports.reverse = reverse;\nfunction expandDims(x, axis) {\n    if (axis === void 0) { axis = -1; }\n    var outShape = shape(x).slice();\n    if (axis < 0) {\n        axis = outShape.length + axis + 1;\n    }\n    outShape.splice(axis, 0, 1);\n    return reshape(x, outShape);\n}\nexports.expandDims = expandDims;\nfunction squeeze(x, axis) {\n    return tfc.squeeze(x, [axis]);\n}\nexports.squeeze = squeeze;\nfunction temporalPadding(x, padding) {\n    if (ndim(x) !== 3) {\n        throw new errors_1.ValueError(\"temporalPadding expects input tensor to be 3-D, but received a \" +\n            (ndim(x) + \"-D tensor.\"));\n    }\n    if (padding == null) {\n        padding = [1, 1];\n    }\n    if (padding.length !== 2) {\n        throw new errors_1.ValueError(\"temporalPadding expects input padding pattern to be a length-2 \" +\n            (\"array, but received a length-\" + padding.length + \" array.\"));\n    }\n    var pattern = [[0, 0], padding, [0, 0]];\n    return tfc.pad(x, pattern);\n}\nexports.temporalPadding = temporalPadding;\nfunction spatial2dPadding(x, padding, dataFormat) {\n    if (ndim(x) !== 4) {\n        throw new errors_1.ValueError(\"temporalPadding expects input tensor to be 4-D, but received a \" +\n            (ndim(x) + \"-D tensor.\"));\n    }\n    if (padding == null) {\n        padding = [[1, 1], [1, 1]];\n    }\n    if (padding.length !== 2 || padding[0].length !== 2 ||\n        padding[1].length !== 2) {\n        throw new errors_1.ValueError('spatial2dPadding expects `padding` to be an Array of two Arrays, ' +\n            'each of which is an Array of two integers.');\n    }\n    if (dataFormat == null) {\n        dataFormat = common_3.imageDataFormat();\n    }\n    if (dataFormat !== 'channelsLast' && dataFormat !== 'channelsFirst') {\n        throw new errors_1.ValueError(\"Unknown data format: \" + dataFormat + \". \" +\n            \"Supported data formats are 'channelsLast' and 'channelsFirst.\");\n    }\n    var pattern;\n    if (dataFormat === 'channelsFirst') {\n        pattern = [[0, 0], [0, 0], padding[0], padding[1]];\n    }\n    else {\n        pattern = [[0, 0], padding[0], padding[1], [0, 0]];\n    }\n    return tfc.pad(x, pattern);\n}\nexports.spatial2dPadding = spatial2dPadding;\nfunction repeat(x, n) {\n    if (x.shape.length !== 2) {\n        throw new errors_1.ValueError(\"repeat() expects a rank-2 tensor, but received a \" +\n            (\"rank-\" + x.shape.length + \" tensor.\"));\n    }\n    var y = expandDims(x, 1);\n    return tile(y, [1, n, 1]);\n}\nexports.repeat = repeat;\nfunction flatten(x) {\n    var newShape = [math_utils.arrayProd(x.shape)];\n    return reshape(x, newShape);\n}\nexports.flatten = flatten;\nfunction batchFlatten(x) {\n    if (ndim(x) <= 1) {\n        throw new errors_1.ValueError(\"batchFlatten requires a minimum rank of 2. Got rank: \" + ndim(x) + \".\");\n    }\n    var newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];\n    return reshape(x, newShape);\n}\nexports.batchFlatten = batchFlatten;\nfunction sliceAlongFirstAxis(array, start, size) {\n    switch (array.rank) {\n        case 1:\n            return tfc.slice1d(array, start, size);\n        case 2:\n            return tfc.slice2d(array, [start, 0], [size, array.shape[1]]);\n        case 3:\n            return tfc.slice3d(array, [start, 0, 0], [size, array.shape[1], array.shape[2]]);\n        case 4:\n            return tfc.slice4d(array, [start, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3]]);\n        default:\n            throw new errors_1.ValueError(\"sliceAlongFirstAxis() received an unsupported tensor rank: \" +\n                (\"\" + array.rank));\n    }\n}\nexports.sliceAlongFirstAxis = sliceAlongFirstAxis;\nfunction sliceAlongLastAxis(array, start, size) {\n    switch (array.rank) {\n        case 1:\n            return tfc.slice1d(array, start, size);\n        case 2:\n            return tfc.slice2d(array, [0, start], [array.shape[0], size]);\n        case 3:\n            return tfc.slice3d(array, [0, 0, start], [array.shape[0], array.shape[1], size]);\n        case 4:\n            return tfc.slice4d(array, [0, 0, 0, start], [array.shape[0], array.shape[1], array.shape[2], size]);\n        default:\n            throw new errors_1.ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \" +\n                (\"\" + array.rank));\n    }\n}\nexports.sliceAlongLastAxis = sliceAlongLastAxis;\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon) {\n    if (epsilon === void 0) { epsilon = 1e-3; }\n    return tfjs_core_1.tidy(function () {\n        var meanAndVariance = tfc.moments(x, reductionAxes);\n        var mean = meanAndVariance.mean;\n        var variance = meanAndVariance.variance;\n        var normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon) {\n    if (epsilon === void 0) { epsilon = 1e-3; }\n    return tfjs_core_1.tidy(function () {\n        var meanAndVariance = tfc.moments(x, reductionAxes);\n        var mean = meanAndVariance.mean;\n        var variance = meanAndVariance.variance;\n        var targetShape = [];\n        for (var _i = 0, _a = math_utils.range(0, ndim(x)); _i < _a.length; _i++) {\n            var axis = _a[_i];\n            if (reductionAxes.indexOf(axis) !== -1) {\n                targetShape.push(1);\n            }\n            else {\n                targetShape.push(x.shape[axis]);\n            }\n        }\n        var broadcastMean = reshape(mean, targetShape);\n        var broadcastVariance = reshape(variance, targetShape);\n        var broadcastGamma = gamma == null ? null : reshape(gamma, targetShape);\n        var broadcastBeta = beta == null ? null : reshape(beta, targetShape);\n        var normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\nfunction normalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon) {\n    if (epsilon === void 0) { epsilon = 1e-3; }\n    if (tfjs_core_1.util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, ndim(x) - 1))) {\n        return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n    else {\n        return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n}\nexports.normalizeBatchInTraining = normalizeBatchInTraining;\nfunction concatenate(tensors, axis) {\n    if (axis === void 0) { axis = -1; }\n    var rank;\n    if (axis < 0) {\n        rank = ndim(tensors[0]);\n        if (rank !== 0) {\n            axis = rank;\n        }\n        else {\n            axis = 0;\n        }\n    }\n    if (axis === ndim(tensors[0])) {\n        axis = -1;\n    }\n    return tfc.concat(tensors, axis);\n}\nexports.concatenate = concatenate;\nfunction concatAlongFirstAxis(a, b) {\n    switch (a.rank) {\n        case 1:\n            return tfc.concat1d([a, b]);\n        case 2:\n            return tfc.concat2d([a, b], 0);\n        case 3:\n            return tfc.concat3d([a, b], 0);\n        case 4:\n            return tfc.concat4d([a, b], 0);\n        default:\n            throw new errors_1.ValueError('concatAlongFirstAxis() received an unsupported tensor rank: ' +\n                a.rank);\n    }\n}\nexports.concatAlongFirstAxis = concatAlongFirstAxis;\nfunction tile(x, n) {\n    if (!Array.isArray(n)) {\n        n = [n];\n    }\n    if (ndim(x) !== n.length) {\n        throw new errors_1.ValueError(\"The length of input n (\" + n.length + \") does not match \" +\n            (\"the number of dimensions in input x (\" + ndim(x) + \")\"));\n    }\n    return tfc.tile(x, n);\n}\nexports.tile = tile;\nfunction variable(x, dtype, name, constraint) {\n    return new types_1.LayerVariable(x, dtype, name, true, constraint);\n}\nexports.variable = variable;\nfunction batchGetValue(xs) {\n    return xs.map(function (x) { return x.read(); });\n}\nexports.batchGetValue = batchGetValue;\nfunction batchSetValue(variablesAndValues) {\n    variablesAndValues.map(function (variableAndValue) {\n        var variable = variableAndValue[0];\n        variable.write(variableAndValue[1]);\n    });\n}\nexports.batchSetValue = batchSetValue;\nfunction zeros(shape, dtype) {\n    return tfc.zeros(shape);\n}\nexports.zeros = zeros;\nfunction zerosVariable(shape, dtype, name) {\n    return new types_1.LayerVariable(zeros(shape), dtype, name);\n}\nexports.zerosVariable = zerosVariable;\nfunction zerosLike(x, dtype, name) {\n    return new types_1.LayerVariable(tfc.zerosLike(x), dtype, name);\n}\nexports.zerosLike = zerosLike;\nfunction ones(shape, dtype) {\n    return tfc.ones(shape);\n}\nexports.ones = ones;\nfunction onesVariable(shape, dtype, name) {\n    var allocated = tfc.ones(shape);\n    return new types_1.LayerVariable(allocated, dtype, name);\n}\nexports.onesVariable = onesVariable;\nfunction onesLike(x, dtype, name) {\n    var allocated = tfc.onesLike(x);\n    return new types_1.LayerVariable(allocated, dtype, name);\n}\nexports.onesLike = onesLike;\nfunction identity(x) {\n    return x.clone();\n}\nexports.identity = identity;\nfunction eye(size, dtype, name) {\n    var buffer = [];\n    for (var i = 0; i < size; ++i) {\n        for (var j = 0; j < size; ++j) {\n            buffer.push(i === j ? 1 : 0);\n        }\n    }\n    return tfjs_core_1.tensor2d(buffer, [size, size]);\n}\nexports.eye = eye;\nfunction eyeVariable(size, dtype, name) {\n    return new types_1.LayerVariable(eye(size, dtype), dtype, name);\n}\nexports.eyeVariable = eyeVariable;\nfunction neg(x) {\n    return tfc.neg(x);\n}\nexports.neg = neg;\nfunction add(x, y) {\n    return tfc.add(x, y);\n}\nexports.add = add;\nfunction subtract(x, y) {\n    return tfc.sub(x, y);\n}\nexports.subtract = subtract;\nfunction multiply(x, y) {\n    return tfc.mul(x, y);\n}\nexports.multiply = multiply;\nfunction divide(x, y) {\n    return tfc.div(x, y);\n}\nexports.divide = divide;\nfunction scalarTimesArray(c, x) {\n    return tfc.mul(c, x);\n}\nexports.scalarTimesArray = scalarTimesArray;\nfunction scalarPlusArray(c, x) {\n    return tfc.add(c, x);\n}\nexports.scalarPlusArray = scalarPlusArray;\nfunction randomUniform(shape, minval, maxval, dtype, seed) {\n    return tfc.randomUniform(shape, minval, maxval);\n}\nexports.randomUniform = randomUniform;\nfunction randomUniformVariable(shape, minval, maxval, dtype, seed, name) {\n    if (name === void 0) { name = 'randomUniform'; }\n    return new types_1.LayerVariable(randomUniform(shape, minval, maxval, dtype, seed), dtype, name);\n}\nexports.randomUniformVariable = randomUniformVariable;\nfunction truncatedNormal(shape, mean, stddev, dtype, seed) {\n    if (mean === void 0) { mean = 0.0; }\n    if (stddev === void 0) { stddev = 1.0; }\n    return tfc.truncatedNormal(shape, mean, stddev);\n}\nexports.truncatedNormal = truncatedNormal;\nfunction truncatedNormalVariable(shape, mean, stddev, dtype, seed, name) {\n    if (mean === void 0) { mean = 0.0; }\n    if (stddev === void 0) { stddev = 1.0; }\n    if (name === void 0) { name = 'truncatedNormal'; }\n    return new types_1.LayerVariable(truncatedNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\nexports.truncatedNormalVariable = truncatedNormalVariable;\nfunction randomNormal(shape, mean, stddev, dtype, seed) {\n    if (mean === void 0) { mean = 0.0; }\n    if (stddev === void 0) { stddev = 1.0; }\n    if (dtype === types_1.DType.bool) {\n        throw new errors_1.NotImplementedError(\"randomNormal does not support dType bool.\");\n    }\n    var dtypeString = (dtype === types_1.DType.float32) ? 'float32' : 'int32';\n    return tfc.randomNormal(shape, mean, stddev, dtypeString, seed);\n}\nexports.randomNormal = randomNormal;\nfunction randomNormalVariable(shape, mean, stddev, dtype, seed, name) {\n    if (mean === void 0) { mean = 0.0; }\n    if (stddev === void 0) { stddev = 1.0; }\n    if (name === void 0) { name = 'randomNormal'; }\n    return new types_1.LayerVariable(randomNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\nexports.randomNormalVariable = randomNormalVariable;\nfunction update(x, xNew) {\n    return x.write(xNew);\n}\nexports.update = update;\nfunction updateAdd(x, increment) {\n    return x.write(tfc.add(x.read(), increment));\n}\nexports.updateAdd = updateAdd;\nfunction updateSub(x, decrement) {\n    return x.write(tfc.sub(x.read(), decrement));\n}\nexports.updateSub = updateSub;\nfunction dot(x, y) {\n    if (ndim(y) !== 2) {\n        throw new errors_1.NotImplementedError(\"dot support for y other than rank 2 is not yet implemented: \" +\n            (\"y shape = \" + shape));\n    }\n    else {\n        if (ndim(x) === 2) {\n            return tfc.matMul(x, y);\n        }\n        else if (ndim(x) === 3) {\n            var xShape0 = x.shape[0];\n            var xShape1 = x.shape[1];\n            var xShape2 = x.shape[2];\n            x = x.reshape([xShape0 * xShape1, xShape2]);\n            return tfc.matMul(x, y).reshape([\n                xShape0, xShape1, y.shape[1]\n            ]);\n        }\n        else {\n            throw new errors_1.NotImplementedError(\"dot support for x of rank \" + ndim(x) + \" is not yet implemented: \" +\n                (\"x shape = \" + shape));\n        }\n    }\n}\nexports.dot = dot;\nfunction sign(x) {\n    var zerosLikeX = tfjs_core_1.zerosLike(x);\n    var onesLikeX = tfjs_core_1.onesLike(x);\n    return tfjs_core_1.where(equal(x, zerosLikeX), zerosLikeX, tfjs_core_1.where(greater(x, tfjs_core_1.zerosLike(x)), onesLikeX, scalarTimesArray(getScalar(-1), onesLikeX)));\n}\nexports.sign = sign;\nfunction qr(x) {\n    if (x.shape.length !== 2) {\n        throw new errors_1.ValueError(\"qr() requires a 2D Tensor, but got a \" + x.shape.length + \"D Tensor.\");\n    }\n    if (x.shape[0] < x.shape[1]) {\n        throw new errors_1.ValueError(\"qr() requires x.shape[0] >= x.shape[1], but got shape: [\" + x.shape + \"]\");\n    }\n    var m = x.shape[0];\n    var n = x.shape[1];\n    var q = eye(m);\n    var r = x;\n    var one2D = tfjs_core_1.tensor2d([[1]], [1, 1]);\n    for (var j = 0; j < n; ++j) {\n        var rjEnd1 = r.slice([j, j], [m - j, 1]);\n        var normX = tfc.norm(rjEnd1);\n        var rjj = r.slice([j, j], [1, 1]);\n        var s = tfc.neg(sign(rjj));\n        var u1 = rjj.sub(multiply(s, normX));\n        var wPre = divide(rjEnd1, u1);\n        var w = void 0;\n        if (wPre.shape[0] === 1) {\n            w = one2D;\n        }\n        else {\n            w = one2D.concat(wPre.slice([1, 0], [wPre.shape[0] - 1, wPre.shape[1]]), 0);\n        }\n        var tau = tfc.neg(divide(tfc.matMul(s, u1), normX));\n        var rjEndAll = r.slice([j, 0], [m - j, n]);\n        var tauTimesW = tau.mul(w);\n        if (j === 0) {\n            r = rjEndAll.sub(tauTimesW.matMul(w.transpose().matMul(rjEndAll)));\n        }\n        else {\n            r = r.slice([0, 0], [j, n])\n                .concat(rjEndAll.sub(tauTimesW.matMul(w.transpose().matMul(rjEndAll))), 0);\n        }\n        var qAllJEnd = q.slice([0, j], [m, q.shape[1] - j]);\n        if (j === 0) {\n            q = qAllJEnd.sub(qAllJEnd.matMul(w).matMul(tauTimesW.transpose()));\n        }\n        else {\n            q = q.slice([0, 0], [m, j])\n                .concat(qAllJEnd.sub(qAllJEnd.matMul(w).matMul(tauTimesW.transpose())), 1);\n        }\n    }\n    return [q, r];\n}\nexports.qr = qr;\nfunction oneHot(indices, numClasses) {\n    if (ndim(indices) !== 1) {\n        throw new Error('Only 1D one-hot tensors are supported in the ' +\n            'deeplearn backend, at present.');\n    }\n    indices = indices.toInt();\n    return tfc.oneHot(indices, numClasses).toFloat();\n}\nexports.oneHot = oneHot;\nfunction mean(x, axis, keepDims) {\n    axis = normalizeAxis(x, axis);\n    return tfc.mean(x, axis, keepDims);\n}\nexports.mean = mean;\nfunction argmax(x, axis) {\n    if (axis === void 0) { axis = -1; }\n    return tfc.argMax(x, axis);\n}\nexports.argmax = argmax;\nfunction gather(reference, indices, axis) {\n    if (Array.isArray(indices)) {\n        indices = tfjs_core_1.tensor1d(indices, 'int32');\n    }\n    else {\n        indices = indices.toInt();\n    }\n    return tfc.gather(reference, indices, axis);\n}\nexports.gather = gather;\nfunction max(x, axis, keepDims) {\n    return tfc.max(x, axis, keepDims);\n}\nexports.max = max;\nfunction min(x, axis, keepDims) {\n    return tfc.min(x, axis, keepDims);\n}\nexports.min = min;\nfunction minimum(x, y) {\n    return tfc.minimum(x, y);\n}\nexports.minimum = minimum;\nfunction sum(x, axis, keepDims) {\n    return tfc.sum(x, axis, keepDims);\n}\nexports.sum = sum;\nfunction abs(x) {\n    return tfc.abs(x);\n}\nexports.abs = abs;\nfunction square(x) {\n    return tfc.mulStrict(x, x);\n}\nexports.square = square;\nfunction sqrt(x) {\n    return tfc.sqrt(x);\n}\nexports.sqrt = sqrt;\nfunction exp(x) {\n    return tfc.exp(x);\n}\nexports.exp = exp;\nfunction log(x) {\n    return tfc.log(x);\n}\nexports.log = log;\nfunction pow(x, a) {\n    if (typeof (a) === 'number') {\n        a = tfjs_core_1.scalar(Math.round(a), 'int32');\n    }\n    if (a.dtype !== 'int32') {\n        throw new errors_1.NotImplementedError(\"Non-int32 dtype (\" + a.dtype + \") is not supported by pow() yet\");\n    }\n    return tfc.pow(x, a);\n}\nexports.pow = pow;\nfunction clip(x, minValue, maxValue) {\n    return tfc.clipByValue(x, minValue, maxValue);\n}\nexports.clip = clip;\nfunction equal(x, y) {\n    return tfc.equal(x, y);\n}\nexports.equal = equal;\nfunction greater(x, y) {\n    return tfc.greater(x, y);\n}\nexports.greater = greater;\nfunction greaterEqual(x, y) {\n    return tfc.greaterEqual(x, y);\n}\nexports.greaterEqual = greaterEqual;\nfunction maximum(x, y) {\n    return tfc.maximum(x, y);\n}\nexports.maximum = maximum;\nfunction sin(x) {\n    return tfc.sin(x.value());\n}\nexports.sin = sin;\nfunction cos(x) {\n    return tfc.cos(x.value());\n}\nexports.cos = cos;\nfunction batchNormalization(x, mean, variance, beta, gamma, epsilon) {\n    if (epsilon === void 0) { epsilon = 1e-3; }\n    var out;\n    if (ndim(x) === 2) {\n        out = tfc.batchNormalization2d(x, mean, variance, epsilon, gamma, beta);\n    }\n    else if (ndim(x) === 3) {\n        out = tfc.batchNormalization3d(x, mean, variance, epsilon, gamma, beta);\n    }\n    else if (ndim(x) === 4) {\n        out = tfc.batchNormalization4d(x, mean, variance, epsilon, gamma, beta);\n    }\n    else {\n        throw new errors_1.NotImplementedError(\"batchNormalization is not implememnted for array of rank \" + ndim(x) + \" \" +\n            \"yet\");\n    }\n    return out;\n}\nexports.batchNormalization = batchNormalization;\nfunction biasAdd(x, bias, dataFormat) {\n    if (dataFormat == null) {\n        dataFormat = common_3.imageDataFormat();\n    }\n    common_1.checkDataFormat(dataFormat);\n    if (ndim(bias) !== 1 && ndim(bias) !== ndim(x)) {\n        throw new errors_1.ValueError('Unexpected bias dimensions: ' + ndim(bias) +\n            '; expected it to be 1 or ' + ndim(x));\n    }\n    var biasShape = bias.shape;\n    var y;\n    if (ndim(x) === 5) {\n        if (dataFormat === 'channelsFirst') {\n            if (biasShape.length === 1) {\n                y = x.add(bias.reshape([1, biasShape[0], 1, 1, 1]));\n            }\n            else {\n                y = x.add(bias.reshape([1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]));\n            }\n        }\n        else if (dataFormat === 'channelsLast') {\n            if (biasShape.length === 1) {\n                y = x.add(bias.reshape([1, 1, 1, 1, biasShape[0]]));\n            }\n            else {\n                y = x.add(bias.reshape([1].concat(biasShape)));\n            }\n        }\n    }\n    else if (ndim(x) === 4) {\n        if (dataFormat === 'channelsFirst') {\n            if (biasShape.length === 1) {\n                y = x.add(bias.reshape([1, biasShape[0], 1, 1]));\n            }\n            else {\n                y = x.add(bias.reshape([1, biasShape[2], biasShape[0], biasShape[1]]));\n            }\n        }\n        else if (dataFormat === 'channelsLast') {\n            if (biasShape.length === 1) {\n                y = x.add(bias.reshape([1, 1, 1, biasShape[0]]));\n            }\n            else {\n                y = x.add(bias.reshape([1].concat(biasShape)));\n            }\n        }\n    }\n    else if (ndim(x) === 3) {\n        if (dataFormat === 'channelsFirst') {\n            if (biasShape.length === 1) {\n                y = x.add(bias.reshape([1, biasShape[0], 1]));\n            }\n            else {\n                y = x.add(bias.reshape([1, biasShape[1], biasShape[0]]));\n            }\n        }\n        else if (dataFormat === 'channelsLast') {\n            if (biasShape.length === 1) {\n                y = x.add(bias.reshape([1, 1, biasShape[0]]));\n            }\n            else {\n                y = x.add(bias.reshape([1].concat(biasShape)));\n            }\n        }\n    }\n    else if (ndim(x) < 3) {\n        y = x.add(bias);\n    }\n    else {\n        throw new errors_1.ValueError(\"Unsupported input rank by biasAdd: \" + ndim(x));\n    }\n    return y;\n}\nexports.biasAdd = biasAdd;\nfunction elu(x, alpha) {\n    if (alpha === void 0) { alpha = 1; }\n    if (alpha !== 1) {\n        throw new errors_1.NotImplementedError(\"Support for alpha values other than 1 (\" + alpha + \") is not implemented \" +\n            \"yet.\");\n    }\n    return tfc.elu(x);\n}\nexports.elu = elu;\nfunction selu(x) {\n    return tfc.selu(x);\n}\nexports.selu = selu;\nfunction relu(x) {\n    return tfc.relu(x);\n}\nexports.relu = relu;\nfunction softplus(x) {\n    return tfc.log(tfc.add(getScalar(1), tfc.exp(x)));\n}\nexports.softplus = softplus;\nfunction softsign(x) {\n    return tfc.div(x, tfc.add(getScalar(1), tfc.abs(x)));\n}\nexports.softsign = softsign;\nfunction tanh(x) {\n    return tfc.tanh(x);\n}\nexports.tanh = tanh;\nfunction dropout(x, level, noiseShape, seed) {\n    if (noiseShape != null && !tfjs_core_1.util.arraysEqual(x.shape, noiseShape)) {\n        throw new errors_1.NotImplementedError('Non-default noise shape is not implemented yet: ' +\n            JSON.stringify(noiseShape));\n    }\n    if (seed != null) {\n        throw new errors_1.NotImplementedError('seed is not implemented for dropout yet.');\n    }\n    var multiplier = tfc.step(tfc.add(neg(level), randomUniform(x.shape, 0, 1, types_1.DType.float32)));\n    multiplier = tfc.mul(divide(getScalar(1), subtract(getScalar(1), level)), multiplier);\n    return tfc.mul(x, multiplier);\n}\nexports.dropout = dropout;\nfunction l2Normalize(x, axis) {\n    var squareSum = sum(square(x), axis, true);\n    var epsilonTensor = scalarTimesArray(tfjs_core_1.scalar(exports.epsilon()), tfc.onesLike(x));\n    var norm = sqrt(maximum(squareSum, epsilonTensor));\n    return divide(x, norm);\n}\nexports.l2Normalize = l2Normalize;\nfunction preprocessConv2DInput(x, dataFormat) {\n    common_1.checkDataFormat(dataFormat);\n    if (dataFormat === 'channelsFirst') {\n        return tfc.transpose(x, [0, 2, 3, 1]);\n    }\n    else {\n        return x;\n    }\n}\nfunction conv1dWithBias(x, kernel, bias, strides, padding, dataFormat, dilationRate) {\n    if (strides === void 0) { strides = 1; }\n    if (padding === void 0) { padding = 'valid'; }\n    if (dilationRate === void 0) { dilationRate = 1; }\n    if (dataFormat == null) {\n        dataFormat = common_3.imageDataFormat();\n    }\n    common_1.checkDataFormat(dataFormat);\n    if (x.shape.length !== 3) {\n        throw new errors_1.ValueError(\"The input of a conv1dWithBias operation should be 3, but is \" +\n            (x.shape.length + \" instead.\"));\n    }\n    if (kernel.shape.length !== 3) {\n        throw new errors_1.ValueError(\"The kernel for a conv1dWithBias operation should be 3, but is \" +\n            (kernel.shape.length + \" instead\"));\n    }\n    if (bias != null && bias.shape.length !== 1) {\n        throw new errors_1.ValueError(\"The bias for a conv1dWithBias operation should be 1, but is \" +\n            (kernel.shape.length + \" instead\"));\n    }\n    if (dataFormat === 'channelsFirst') {\n        x = transpose(x, [0, 2, 1]);\n    }\n    if (padding === 'casual') {\n        throw new errors_1.NotImplementedError('The support for CASUAL padding mode in conv1dWithBias is not ' +\n            'implemented yet.');\n    }\n    var y = tfc.conv1d(x, kernel, strides, padding === 'same' ? 'same' : 'valid', 'NWC', dilationRate);\n    if (bias != null) {\n        y = biasAdd(y, bias);\n    }\n    return y;\n}\nexports.conv1dWithBias = conv1dWithBias;\nfunction conv1d(x, kernel, strides, padding, dataFormat, dilationRate) {\n    if (strides === void 0) { strides = 1; }\n    if (padding === void 0) { padding = 'valid'; }\n    if (dilationRate === void 0) { dilationRate = 1; }\n    common_1.checkDataFormat(dataFormat);\n    return conv1dWithBias(x, kernel, null, strides, padding, dataFormat, dilationRate);\n}\nexports.conv1d = conv1d;\nfunction conv2d(x, kernel, strides, padding, dataFormat, dilationRate) {\n    if (strides === void 0) { strides = [1, 1]; }\n    if (padding === void 0) { padding = 'valid'; }\n    common_1.checkDataFormat(dataFormat);\n    return conv2dWithBias(x, kernel, null, strides, padding, dataFormat, dilationRate);\n}\nexports.conv2d = conv2d;\nfunction conv2dWithBias(x, kernel, bias, strides, padding, dataFormat, dilationRate) {\n    if (strides === void 0) { strides = [1, 1]; }\n    if (padding === void 0) { padding = 'valid'; }\n    if (dataFormat == null) {\n        dataFormat = common_3.imageDataFormat();\n    }\n    common_1.checkDataFormat(dataFormat);\n    if (ndim(x) !== 3 && ndim(x) !== 4) {\n        throw new errors_1.ValueError(\"conv2dWithBias expects input to be of rank 3 or 4, but received \" +\n            (ndim(x) + \".\"));\n    }\n    if (ndim(kernel) !== 3 && ndim(kernel) !== 4) {\n        throw new errors_1.ValueError(\"conv2dWithBias expects kernel to be of rank 3 or 4, but received \" +\n            (ndim(x) + \".\"));\n    }\n    var y = preprocessConv2DInput(x, dataFormat);\n    if (padding === 'casual') {\n        throw new errors_1.NotImplementedError('The support for CASUAL padding mode in conv1dWithBias is not ' +\n            'implemented yet.');\n    }\n    y = tfc.conv2d(y, kernel, strides, padding === 'same' ? 'same' : 'valid', 'NHWC', dilationRate);\n    if (bias != null) {\n        y = biasAdd(y, bias);\n    }\n    if (dataFormat === 'channelsFirst') {\n        y = tfc.transpose(y, [0, 3, 1, 2]);\n    }\n    return y;\n}\nexports.conv2dWithBias = conv2dWithBias;\nfunction depthwiseConv2d(x, depthwiseKernel, strides, padding, dataFormat, dilationRate) {\n    if (strides === void 0) { strides = [1, 1]; }\n    if (padding === void 0) { padding = 'valid'; }\n    if (dataFormat == null) {\n        dataFormat = common_3.imageDataFormat();\n    }\n    common_1.checkDataFormat(dataFormat);\n    var y = preprocessConv2DInput(x, dataFormat);\n    if (ndim(x) !== 4) {\n        throw new errors_1.ValueError(\"Input for depthwiseConv2d is required to be 4-D, but is instead \" +\n            (ndim(x) + \"-D\"));\n    }\n    if (ndim(depthwiseKernel) !== 4) {\n        throw new errors_1.ValueError(\"depthwiseKernel is required to be 4-D, but is instead \" +\n            (ndim(depthwiseKernel) + \"-D\"));\n    }\n    y = tfc.depthwiseConv2d(y, depthwiseKernel, strides, padding === 'same' ? 'same' : 'valid', 'NHWC', dilationRate);\n    if (dataFormat === 'channelsFirst') {\n        y = tfc.transpose(y, [0, 3, 1, 2]);\n    }\n    return y;\n}\nexports.depthwiseConv2d = depthwiseConv2d;\nfunction pool2d(x, poolSize, strides, padding, dataFormat, poolMode) {\n    common_1.checkDataFormat(dataFormat);\n    common_1.checkPoolMode(poolMode);\n    common_1.checkPaddingMode(padding);\n    if (strides == null) {\n        strides = [1, 1];\n    }\n    if (padding == null) {\n        padding = 'valid';\n    }\n    if (dataFormat == null) {\n        dataFormat = common_3.imageDataFormat();\n    }\n    if (poolMode == null) {\n        poolMode = 'max';\n    }\n    x = preprocessConv2DInput(x, dataFormat);\n    var y;\n    var paddingString = (padding === 'same') ? 'same' : 'valid';\n    if (poolMode === 'max') {\n        y = tfc.maxPool(x, poolSize, strides, paddingString);\n    }\n    else {\n        y = tfc.avgPool(x, poolSize, strides, paddingString);\n    }\n    if (dataFormat === 'channelsFirst') {\n        y = tfc.transpose(y, [0, 3, 1, 2]);\n    }\n    return y;\n}\nexports.pool2d = pool2d;\nfunction nameScope(name, fn) {\n    return common_1.nameScope(name, fn);\n}\nexports.nameScope = nameScope;\nfunction floatx() {\n    return types_1.DType.float32;\n}\nexports.floatx = floatx;\nvar _uidPrefixes = {};\nfunction getUid(prefix) {\n    if (prefix === void 0) { prefix = ''; }\n    if (!(prefix in _uidPrefixes)) {\n        _uidPrefixes[prefix] = 0;\n    }\n    _uidPrefixes[prefix] += 1;\n    return prefix + _uidPrefixes[prefix].toString();\n}\nexports.getUid = getUid;\nfunction softmax(x, axis) {\n    if (axis === void 0) { axis = -1; }\n    return tfc.softmax(x, axis);\n}\nexports.softmax = softmax;\nfunction categoricalCrossentropy(target, output, fromLogits) {\n    if (fromLogits === void 0) { fromLogits = false; }\n    if (fromLogits) {\n        output = softmax(output);\n    }\n    else {\n        var outputSum = sum(output, shape(output).length - 1, true);\n        output = divide(output, outputSum);\n    }\n    output = clip(output, exports.epsilon(), 1 - exports.epsilon());\n    return tfc.neg(tfc.sum(tfc.mul(target.toFloat(), tfc.log(output)), shape(output).length - 1));\n}\nexports.categoricalCrossentropy = categoricalCrossentropy;\nfunction sparseCategoricalCrossentropy(target, output, fromLogits) {\n    if (fromLogits === void 0) { fromLogits = false; }\n    var flatTarget = tfc.floor(flatten(target)).toInt();\n    var outputShape = shape(output);\n    var oneHotTarget = reshape(tfc.oneHot(flatTarget, outputShape[outputShape.length - 1]), outputShape);\n    return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n}\nexports.sparseCategoricalCrossentropy = sparseCategoricalCrossentropy;\nfunction binaryCrossentropy(target, output, fromLogits) {\n    if (fromLogits === void 0) { fromLogits = false; }\n    var y;\n    if (!fromLogits) {\n        y = clip(output, exports.epsilon(), 1 - exports.epsilon());\n        y = log(divide(y, subtract(tfc.onesLike(y), y)));\n    }\n    else {\n        y = output;\n    }\n    return sigmoidCrossEntropyWithLogits(target, y);\n}\nexports.binaryCrossentropy = binaryCrossentropy;\nfunction sigmoidCrossEntropyWithLogits(target, output) {\n    var maxOutput = tfc.maximum(output, tfc.zerosLike(output));\n    var outputXTarget = tfc.mul(output, target);\n    var sigmoidOutput = tfc.log(tfc.add(getScalar(1), tfc.exp(tfc.neg(tfc.abs(output)))));\n    var result = tfc.add(tfc.sub(maxOutput, outputXTarget), sigmoidOutput);\n    return result;\n}\nexports.sigmoidCrossEntropyWithLogits = sigmoidCrossEntropyWithLogits;\nfunction sigmoid(x) {\n    return tfc.sigmoid(x);\n}\nexports.sigmoid = sigmoid;\nfunction hardSigmoid(x) {\n    var y = scalarPlusArray(tfjs_core_1.scalar(0.5), scalarTimesArray(tfjs_core_1.scalar(0.2), x));\n    return clip(y, 0, 1);\n}\nexports.hardSigmoid = hardSigmoid;\nfunction inTrainPhase(x, alt, training) {\n    if (training === void 0) { training = false; }\n    return training ? x() : alt();\n}\nexports.inTrainPhase = inTrainPhase;\nfunction rnn(stepFunction, inputs, initialStates, goBackwards, mask, constants, unroll, inputLength) {\n    if (goBackwards === void 0) { goBackwards = false; }\n    if (unroll === void 0) { unroll = false; }\n    var ndim = inputs.shape.length;\n    if (ndim < 3) {\n        throw new errors_1.ValueError(\"Input should be at least 3D, but is \" + ndim + \"D.\");\n    }\n    var axes = [1, 0].concat(math_utils.range(2, ndim));\n    inputs = transpose(inputs, axes);\n    if (mask != null) {\n        throw new errors_1.NotImplementedError('The rnn() function of the deeplearn.js backend does not support ' +\n            'masking yet.');\n    }\n    if (constants != null) {\n        throw new errors_1.NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' +\n            'constants yet.');\n    }\n    if (unroll) {\n        console.warn('Backend rnn(): the unroll = true option is not applicable to the ' +\n            'imperative deeplearn.js backend.');\n    }\n    if (goBackwards) {\n        inputs = reverse(inputs, 0);\n    }\n    var outputs;\n    var lastOutput;\n    var states = initialStates;\n    var timeSteps = inputs.shape[0];\n    for (var t = 0; t < timeSteps; ++t) {\n        var currentInput = sliceAlongFirstAxis(inputs, t, 1);\n        currentInput = reshape(currentInput, currentInput.shape.slice(1));\n        var stepOutputs = stepFunction(currentInput, states);\n        lastOutput = stepOutputs[0];\n        if (t === 0) {\n            outputs = lastOutput.reshape([1].concat(lastOutput.shape));\n        }\n        else {\n            outputs = concatAlongFirstAxis(outputs, lastOutput.reshape([1].concat(lastOutput.shape)));\n        }\n        states = stepOutputs[1];\n    }\n    return [\n        lastOutput,\n        transpose(outputs, [1, 0].concat(math_utils.range(2, outputs.shape.length))),\n        states\n    ];\n}\nexports.rnn = rnn;\nfunction gradients(lossFn, variables) {\n    var variableList = variables.map(function (variable) { return variable.read(); });\n    var valudAndGrads = tfjs_core_1.variableGrads(lossFn, variableList);\n    return variables.map(function (variable) { return valudAndGrads.grads[variable.name]; });\n}\nexports.gradients = gradients;\n"},"hash":"37603b0fc2718bbc51664c4b94a0af4b","cacheData":{"env":{}}}