{"dependencies":[{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\package.json","includedInParent":true,"mtime":1528200878026},{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\.babelrc","includedInParent":true,"mtime":1528197961732},{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1524501157000},{"name":"./backend/tfjs_backend","loc":{"line":3,"column":16}},{"name":"./errors","loc":{"line":4,"column":23}}],"generated":{"js":"\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar K = require(\"./backend/tfjs_backend\");\nvar errors_1 = require(\"./errors\");\nfunction meanSquaredError(yTrue, yPred) {\n    return K.mean(K.square(K.subtract(yPred, yTrue)), -1);\n}\nexports.meanSquaredError = meanSquaredError;\nfunction meanAbsoluteError(yTrue, yPred) {\n    return K.mean(K.abs(K.subtract(yPred, yTrue)), -1);\n}\nexports.meanAbsoluteError = meanAbsoluteError;\nfunction meanAbsolutePercentageError(yTrue, yPred) {\n    var diff = K.subtract(yTrue, yPred);\n    var clippedTrue = K.clip(K.abs(yTrue), K.epsilon(), Number.MAX_VALUE);\n    var absResult = K.abs(K.divide(diff, clippedTrue));\n    return K.scalarTimesArray(K.getScalar(100.0), K.mean(absResult, -1));\n}\nexports.meanAbsolutePercentageError = meanAbsolutePercentageError;\nfunction meanSquaredLogarithmicError(yTrue, yPred) {\n    var one = K.getScalar(1.0);\n    var clippedPred = K.clip(yPred, K.epsilon(), Number.MAX_VALUE);\n    var firstLog = K.log(K.scalarPlusArray(one, clippedPred));\n    var clippedTrue = K.clip(yTrue, K.epsilon(), Number.MAX_VALUE);\n    var secondLog = K.log(K.scalarPlusArray(one, clippedTrue));\n    return K.mean(K.square(K.subtract(firstLog, secondLog)), -1);\n}\nexports.meanSquaredLogarithmicError = meanSquaredLogarithmicError;\nfunction squaredHinge(yTrue, yPred) {\n    var zeroTensor = K.getScalar(0.0);\n    var one = K.getScalar(1.0);\n    var maxResult = K.maximum(zeroTensor, K.subtract(one, K.multiply(yTrue, yPred)));\n    return K.mean(K.square(maxResult), -1);\n}\nexports.squaredHinge = squaredHinge;\nfunction hinge(yTrue, yPred) {\n    var zeroTensor = K.getScalar(0.0);\n    var one = K.getScalar(1.0);\n    var maxResult = K.maximum(zeroTensor, K.subtract(one, K.multiply(yTrue, yPred)));\n    return K.mean(maxResult, -1);\n}\nexports.hinge = hinge;\nfunction categoricalHinge(yTrue, yPred) {\n    var zeroTensor = K.getScalar(0.0);\n    var one = K.getScalar(1.0);\n    var pos = K.sum(K.multiply(yTrue, yPred), -1);\n    var neg = K.max(K.multiply(K.subtract(one, yTrue), yPred), -1);\n    return K.maximum(zeroTensor, K.scalarPlusArray(one, K.subtract(neg, pos)));\n}\nexports.categoricalHinge = categoricalHinge;\nfunction logcosh(yTrue, yPred) {\n    var log2 = K.getScalar(Math.log(2.0));\n    var predictionDiff = K.subtract(yPred, yTrue);\n    var logcoshResult = K.subtract(K.add(predictionDiff, K.softplus(K.scalarTimesArray(K.getScalar(-2.0), predictionDiff))), log2);\n    return K.mean(logcoshResult, -1);\n}\nexports.logcosh = logcosh;\nfunction categoricalCrossentropy(yTrue, yPred) {\n    return K.categoricalCrossentropy(yTrue, yPred);\n}\nexports.categoricalCrossentropy = categoricalCrossentropy;\nfunction sparseCategoricalCrossentropy(yTrue, yPred) {\n    return K.sparseCategoricalCrossentropy(yTrue, yPred);\n}\nexports.sparseCategoricalCrossentropy = sparseCategoricalCrossentropy;\nfunction binaryCrossentropy(yTrue, yPred) {\n    return K.mean(K.binaryCrossentropy(yTrue, yPred), -1);\n}\nexports.binaryCrossentropy = binaryCrossentropy;\nfunction kullbackLeiblerDivergence(yTrue, yPred) {\n    var clippedTrue = K.clip(yTrue, K.epsilon(), 1);\n    var clippedPred = K.clip(yPred, K.epsilon(), 1);\n    return K.sum(K.multiply(yTrue, K.log(K.divide(clippedTrue, clippedPred))), -1);\n}\nexports.kullbackLeiblerDivergence = kullbackLeiblerDivergence;\nfunction poisson(yTrue, yPred) {\n    var logPred = K.log(K.scalarPlusArray(K.getScalar(K.epsilon()), yPred));\n    return K.mean(K.subtract(yPred, K.multiply(yTrue, logPred)), -1);\n}\nexports.poisson = poisson;\nfunction cosineProximity(yTrue, yPred) {\n    var trueNormalized = K.l2Normalize(yTrue, -1);\n    var predNormalized = K.l2Normalize(yPred, -1);\n    var trueXPred = K.multiply(trueNormalized, predNormalized);\n    return K.neg(K.sum(trueXPred, -1));\n}\nexports.cosineProximity = cosineProximity;\nexports.mse = meanSquaredError;\nexports.MSE = meanSquaredError;\nexports.mae = meanAbsoluteError;\nexports.MAE = meanAbsoluteError;\nexports.mape = meanAbsolutePercentageError;\nexports.MAPE = meanAbsolutePercentageError;\nexports.msle = meanSquaredLogarithmicError;\nexports.MSLE = meanSquaredLogarithmicError;\nexports.kld = kullbackLeiblerDivergence;\nexports.KLD = kullbackLeiblerDivergence;\nexports.cosine = cosineProximity;\nfunction get(identifierOrFn) {\n    var lossesMap = {\n        meanSquaredError: meanSquaredError,\n        meanAbsoluteError: meanAbsoluteError,\n        meanAbsolutePercentageError: meanAbsolutePercentageError,\n        meanSquaredLogarithmicError: meanSquaredLogarithmicError,\n        squaredHinge: squaredHinge,\n        hinge: hinge,\n        categoricalHinge: categoricalHinge,\n        logcosh: logcosh,\n        categoricalCrossentropy: categoricalCrossentropy,\n        sparseCategoricalCrossentropy: sparseCategoricalCrossentropy,\n        binaryCrossentropy: binaryCrossentropy,\n        kullbackLeiblerDivergence: kullbackLeiblerDivergence,\n        poisson: poisson,\n        cosineProximity: cosineProximity\n    };\n    if (typeof identifierOrFn === 'string') {\n        if (identifierOrFn in lossesMap) {\n            return lossesMap[identifierOrFn];\n        }\n        throw new errors_1.ValueError(\"Unknown loss \" + identifierOrFn);\n    }\n    else {\n        return identifierOrFn;\n    }\n}\nexports.get = get;\n"},"hash":"05f93f37bbc55cdbd68953f333443ca7","cacheData":{"env":{}}}